{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6911f882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1.9565, Validation Loss: 1.3555\n",
      "Epoch 2/100, Train Loss: 0.9383, Validation Loss: 0.6811\n",
      "Epoch 3/100, Train Loss: 0.4311, Validation Loss: 0.3691\n",
      "Epoch 4/100, Train Loss: 0.3274, Validation Loss: 0.3780\n",
      "Epoch 5/100, Train Loss: 0.2299, Validation Loss: 0.2335\n",
      "Epoch 6/100, Train Loss: 0.1917, Validation Loss: 0.2485\n",
      "Epoch 7/100, Train Loss: 0.1829, Validation Loss: 0.2478\n",
      "Epoch 8/100, Train Loss: 0.1259, Validation Loss: 0.2070\n",
      "Epoch 9/100, Train Loss: 0.0886, Validation Loss: 0.2679\n",
      "Epoch 10/100, Train Loss: 0.1499, Validation Loss: 0.2634\n",
      "Epoch 11/100, Train Loss: 0.1093, Validation Loss: 0.3260\n",
      "Epoch 12/100, Train Loss: 0.1434, Validation Loss: 0.2984\n",
      "Epoch 13/100, Train Loss: 0.0659, Validation Loss: 0.2419\n",
      "Epoch 14/100, Train Loss: 0.0863, Validation Loss: 0.2262\n",
      "Epoch 15/100, Train Loss: 0.1038, Validation Loss: 0.2606\n",
      "Epoch 16/100, Train Loss: 0.0581, Validation Loss: 0.3508\n",
      "Epoch 17/100, Train Loss: 0.0795, Validation Loss: 0.2350\n",
      "Epoch 18/100, Train Loss: 0.0446, Validation Loss: 0.1943\n",
      "Epoch 19/100, Train Loss: 0.0381, Validation Loss: 0.2483\n",
      "Epoch 20/100, Train Loss: 0.0296, Validation Loss: 0.2265\n",
      "Epoch 21/100, Train Loss: 0.0313, Validation Loss: 0.2305\n",
      "Epoch 22/100, Train Loss: 0.0193, Validation Loss: 0.2497\n",
      "Epoch 23/100, Train Loss: 0.0267, Validation Loss: 0.2927\n",
      "Epoch 24/100, Train Loss: 0.0784, Validation Loss: 0.2815\n",
      "Epoch 25/100, Train Loss: 0.1039, Validation Loss: 0.3320\n",
      "Epoch 26/100, Train Loss: 0.0731, Validation Loss: 0.3211\n",
      "Epoch 27/100, Train Loss: 0.0826, Validation Loss: 0.3206\n",
      "Epoch 28/100, Train Loss: 0.1540, Validation Loss: 0.2914\n",
      "Epoch 29/100, Train Loss: 0.1849, Validation Loss: 0.3107\n",
      "Epoch 30/100, Train Loss: 0.0817, Validation Loss: 0.3223\n",
      "Epoch 31/100, Train Loss: 0.0410, Validation Loss: 0.2442\n",
      "Epoch 32/100, Train Loss: 0.0315, Validation Loss: 0.2374\n",
      "Epoch 33/100, Train Loss: 0.0463, Validation Loss: 0.2704\n",
      "Epoch 34/100, Train Loss: 0.0423, Validation Loss: 0.2068\n",
      "Epoch 35/100, Train Loss: 0.0393, Validation Loss: 0.2633\n",
      "Epoch 36/100, Train Loss: 0.0634, Validation Loss: 0.3072\n",
      "Epoch 37/100, Train Loss: 0.0979, Validation Loss: 0.2615\n",
      "Epoch 38/100, Train Loss: 0.0803, Validation Loss: 0.2266\n",
      "Epoch 39/100, Train Loss: 0.0325, Validation Loss: 0.2826\n",
      "Epoch 40/100, Train Loss: 0.0258, Validation Loss: 0.2191\n",
      "Epoch 41/100, Train Loss: 0.0367, Validation Loss: 0.3144\n",
      "Epoch 42/100, Train Loss: 0.0494, Validation Loss: 0.2452\n",
      "Epoch 43/100, Train Loss: 0.0345, Validation Loss: 0.2382\n",
      "Epoch 44/100, Train Loss: 0.0184, Validation Loss: 0.3055\n",
      "Epoch 45/100, Train Loss: 0.0579, Validation Loss: 0.2740\n",
      "Epoch 46/100, Train Loss: 0.0418, Validation Loss: 0.3289\n",
      "Epoch 47/100, Train Loss: 0.0408, Validation Loss: 0.2791\n",
      "Epoch 48/100, Train Loss: 0.0221, Validation Loss: 0.2949\n",
      "Epoch 49/100, Train Loss: 0.0464, Validation Loss: 0.2430\n",
      "Epoch 50/100, Train Loss: 0.0822, Validation Loss: 0.2554\n",
      "Epoch 51/100, Train Loss: 0.0600, Validation Loss: 0.2637\n",
      "Epoch 52/100, Train Loss: 0.0827, Validation Loss: 0.2994\n",
      "Epoch 53/100, Train Loss: 0.0422, Validation Loss: 0.4114\n",
      "Epoch 54/100, Train Loss: 0.1114, Validation Loss: 0.3374\n",
      "Epoch 55/100, Train Loss: 0.1378, Validation Loss: 0.5188\n",
      "Epoch 56/100, Train Loss: 0.1187, Validation Loss: 0.2416\n",
      "Epoch 57/100, Train Loss: 0.0967, Validation Loss: 0.2478\n",
      "Epoch 58/100, Train Loss: 0.0728, Validation Loss: 0.2530\n",
      "Epoch 59/100, Train Loss: 0.0496, Validation Loss: 0.2706\n",
      "Epoch 60/100, Train Loss: 0.1405, Validation Loss: 0.2480\n",
      "Epoch 61/100, Train Loss: 0.0768, Validation Loss: 0.3247\n",
      "Epoch 62/100, Train Loss: 0.0622, Validation Loss: 0.2617\n",
      "Epoch 63/100, Train Loss: 0.0665, Validation Loss: 0.2844\n",
      "Epoch 64/100, Train Loss: 0.0483, Validation Loss: 0.2558\n",
      "Epoch 65/100, Train Loss: 0.0412, Validation Loss: 0.2871\n",
      "Epoch 66/100, Train Loss: 0.0504, Validation Loss: 0.3040\n",
      "Epoch 67/100, Train Loss: 0.0230, Validation Loss: 0.3384\n",
      "Epoch 68/100, Train Loss: 0.0296, Validation Loss: 0.3161\n",
      "Epoch 69/100, Train Loss: 0.0399, Validation Loss: 0.2920\n",
      "Epoch 70/100, Train Loss: 0.0249, Validation Loss: 0.2052\n",
      "Epoch 71/100, Train Loss: 0.0287, Validation Loss: 0.2473\n",
      "Epoch 72/100, Train Loss: 0.0260, Validation Loss: 0.2786\n",
      "Epoch 73/100, Train Loss: 0.0347, Validation Loss: 0.2810\n",
      "Epoch 74/100, Train Loss: 0.0268, Validation Loss: 0.2847\n",
      "Epoch 75/100, Train Loss: 0.0457, Validation Loss: 0.3185\n",
      "Epoch 76/100, Train Loss: 0.0547, Validation Loss: 0.4000\n",
      "Epoch 77/100, Train Loss: 0.0624, Validation Loss: 0.3205\n",
      "Epoch 78/100, Train Loss: 0.1011, Validation Loss: 0.3625\n",
      "Epoch 79/100, Train Loss: 0.0927, Validation Loss: 0.2403\n",
      "Epoch 80/100, Train Loss: 0.0486, Validation Loss: 0.2443\n",
      "Epoch 81/100, Train Loss: 0.0252, Validation Loss: 0.2101\n",
      "Epoch 82/100, Train Loss: 0.0303, Validation Loss: 0.2043\n",
      "Epoch 83/100, Train Loss: 0.0671, Validation Loss: 0.2160\n",
      "Epoch 84/100, Train Loss: 0.0509, Validation Loss: 0.2379\n",
      "Epoch 85/100, Train Loss: 0.0336, Validation Loss: 0.2483\n",
      "Epoch 86/100, Train Loss: 0.0194, Validation Loss: 0.3148\n",
      "Epoch 87/100, Train Loss: 0.0298, Validation Loss: 0.2569\n",
      "Epoch 88/100, Train Loss: 0.0759, Validation Loss: 0.2537\n",
      "Epoch 89/100, Train Loss: 0.0770, Validation Loss: 0.3061\n",
      "Epoch 90/100, Train Loss: 0.0957, Validation Loss: 0.2645\n",
      "Epoch 91/100, Train Loss: 0.0442, Validation Loss: 0.2905\n",
      "Epoch 92/100, Train Loss: 0.0944, Validation Loss: 0.2051\n",
      "Epoch 93/100, Train Loss: 0.0640, Validation Loss: 0.1853\n",
      "Epoch 94/100, Train Loss: 0.0692, Validation Loss: 0.2516\n",
      "Epoch 95/100, Train Loss: 0.0685, Validation Loss: 0.2698\n",
      "Epoch 96/100, Train Loss: 0.0471, Validation Loss: 0.3415\n",
      "Epoch 97/100, Train Loss: 0.0895, Validation Loss: 0.2152\n",
      "Epoch 98/100, Train Loss: 0.0461, Validation Loss: 0.2972\n",
      "Epoch 99/100, Train Loss: 0.0494, Validation Loss: 0.4463\n",
      "Epoch 100/100, Train Loss: 0.0794, Validation Loss: 0.1979\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class GurmukhiDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "def load_images_from_folder(base_folder):\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for digit in range(10):\n",
    "        folder_path = os.path.join(base_folder, str(digit))\n",
    "        for filename in os.listdir(folder_path):\n",
    "            img = cv2.imread(os.path.join(folder_path, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                X_data.append(img.flatten())\n",
    "                y_data.append(digit)\n",
    "\n",
    "    return np.array(X_data), np.array(y_data)\n",
    "\n",
    "\n",
    "# Load the data\n",
    "X_train_data, y_train_data = load_images_from_folder(\"train\")\n",
    "X_val_data, y_val_data = load_images_from_folder(\"val\")\n",
    "\n",
    "# Normalize the data\n",
    "X_train_data = X_train_data / 255.0\n",
    "X_val_data = X_val_data / 255.0\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = GurmukhiDataset(X_train_data, y_train_data)\n",
    "val_dataset = GurmukhiDataset(X_val_data, y_val_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "\n",
    "class gurmukhi_digit_classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.hidden2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.hidden3 = nn.Linear(hidden_size[1], hidden_size[2])\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_size[2], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "input_size = X_train_data.shape[1]\n",
    "hidden_size = [128, 64, 32]\n",
    "output_size = 10\n",
    "dropout_p = 0.4  # Increased dropout probability\n",
    "\n",
    "model = gurmukhi_digit_classifier(input_size, hidden_size, output_size, dropout_p)\n",
    "\n",
    "# # Add weight decay to the optimizer for L2 regularization (increased value)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3, weight_decay=9e-4)\n",
    "\n",
    "# Train the neural network\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images\n",
    "        labels = labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images\n",
    "            labels = labels\n",
    "\n",
    "            output = model(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "84b2a863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gurmukhi_digit_classifier(\n",
       "  (hidden1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (hidden3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e59afcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def preprocess_image_unseeen(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    resized_image = cv2.resize(image, (32, 32))\n",
    "    image_array = np.array(resized_image).flatten()\n",
    "    image_array = image_array / 255.0\n",
    "    return image_array\n",
    "\n",
    "def predict(image_path, model):\n",
    "    model.eval()\n",
    "    image_array = preprocess_image_unseeen(image_path)\n",
    "    input_tensor = torch.tensor(image_array, dtype=torch.float32).unsqueeze(0)\n",
    "    output = model(input_tensor)\n",
    "    _, predicted_digit = torch.max(output, 1)\n",
    "    return predicted_digit.item()\n",
    "\n",
    "# Example usage\n",
    "image_path = './val/8/6.tiff'\n",
    "predicted_digit = predict(image_path, model)\n",
    "print(f\"Predicted digit: {predicted_digit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7028e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
